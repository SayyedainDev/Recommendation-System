import pandas as pd
import numpy as np
import joblib
import pyrebase
import traceback
import uuid # For unique interaction IDs
from datetime import datetime
import random # For simulating random data

# ------------------- Firebase Configuration -------------------
# IMPORTANT: Replace these with your actual Firebase project credentials
firebase_config = {
    "apiKey": "YOUR_API_KEY",
    "authDomain": "pet-adoption-69346.firebaseapp.com",
    "databaseURL": "https://pet-adoption/",
    "projectId": "pet-adoption-69346",
    "storageBucket": "pet-adoption-69346.appspot.com",
    "messagingSenderId": "YOUR_SENDER_ID",
    "appId": "YOUR_APP_ID"
}

firebase = pyrebase.initialize_app(firebase_config)
db = firebase.database()

# ------------------- Load Model and Transformers -------------------
# These files are expected to be generated by your 'model_training.py' script.
# Ensure 'model_training.py' has been run and these files are in the same directory.
try:
    model = joblib.load("best_model.pkl")
    encoder = joblib.load("encoder.pkl")
    scaler = joblib.load("scaler.pkl")
    categorical_cols = joblib.load("categorical_columns.pkl")
    numerical_cols = joblib.load("numerical_columns.pkl")
    print("Successfully loaded model and transformers.")
except FileNotFoundError as e:
    print(f"Error: Required model/transformer file not found: {e}. "
          "Please ensure 'model_training.py' has been run to generate them.")
    exit() # Exit if essential files is missing

# ------------------- Firebase Data Population Functions -------------------

def clear_firebase_data():
    """Clears all data from 'users', 'pets', and 'user_interactions' nodes.
    Use with extreme caution, especially in production environments!"""
    print("Clearing existing Firebase data (users, pets, interactions)...")
    db.child("users").remove()
    db.child("pets").remove()
    db.child("user_interactions").remove()
    print("Firebase data cleared.")

def populate_sample_users(num_users=15):
    """Populates Firebase with sample user data."""
    print(f"Populating {num_users} sample users...")
    for i in range(1, num_users + 1):
        user_id = f"user{i:03d}"
        user_data = {
            "name": f"User {i}",
            "email": f"user{i}@example.com"
        }
        db.child("users").child(user_id).set(user_data)
    print("Sample users populated.")

def populate_sample_pets(pets_df):
    """Populates Firebase with sample pet data from the DataFrame."""
    print(f"Populating {len(pets_df)} sample pets from DataFrame...")
    for index, row in pets_df.iterrows():
        pet_id = str(index) # Use the DataFrame index as pet_id
        pet_data = row.to_dict()
        # Ensure 'features' is stored as a list if it's a string representation of a list
        if isinstance(pet_data.get('features'), str):
            try:
                pet_data['features'] = eval(pet_data['features']) # Safely evaluate string to list
            except (SyntaxError, NameError):
                pet_data['features'] = [] # Default to empty list if evaluation fails
        db.child("pets").child(pet_id).set(pet_data)
    print("Sample pets populated.")

def populate_sample_interactions(sample_users_list, sample_pets_list, num_interactions=100, test_user_id="user007"):
    """
    Populates Firebase with sample user interaction data.
    Ensures specific, diverse interactions for the test_user_id.
    """
    print(f"Populating {num_interactions} sample interactions...")
    interaction_types = ["view", "like", "inquiry", "adopted"]

    # Ensure test_user_id exists in the sample_users_list
    if test_user_id not in sample_users_list:
        sample_users_list.append(test_user_id)

    # --- Clear specific interactions for the test user first ---
    # This ensures a clean slate for controlled testing
    user_interactions_ref = db.child("user_interactions").get().val()
    if user_interactions_ref:
        for interaction_key, interaction_data in list(user_interactions_ref.items()): # Use list() to allow modification during iteration
            if interaction_data.get("user_id") == test_user_id:
                db.child("user_interactions").child(interaction_key).remove()
    print(f"Cleared existing interactions for {test_user_id}.")

    # Define a list of diverse pets that the test user should interact with to influence recommendations
    # These pet IDs should ideally exist in your pets_for_firebase (from pet_adoption_data.csv)
    # Let's ensure a mix of types that are NOT rabbits
    diverse_pets_for_test_user = [
        "pet003", "pet004", "pet014", "pet027", "pet031", "pet039", "pet042", "pet078",
        "pet001", "pet005", "pet006", "pet007" # Adding more diverse, non-rabbit pets
    ]

    # Filter to only include pets that are actually in sample_pets_list (from pet_adoption_data.csv)
    diverse_pets_for_test_user = [p for p in diverse_pets_for_test_user if p in sample_pets_list]

    # Ensure the test user has strong positive interactions with these diverse pets
    specific_interactions_count = 0
    print(f"\nAdding specific interactions for {test_user_id}:")
    for pet_id in diverse_pets_for_test_user:
        # Add a 'like' and a 'view' interaction for a stronger signal
        db.child("user_interactions").child(str(uuid.uuid4())).set({
            "user_id": test_user_id,
            "pet_id": pet_id,
            "interaction_type": "like",
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })
        db.child("user_interactions").child(str(uuid.uuid4())).set({
            "user_id": test_user_id,
            "pet_id": pet_id,
            "interaction_type": "view",
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })
        specific_interactions_count += 2
        print(f"  - User {test_user_id} liked and viewed Pet {pet_id}")

    # Add general random interactions for other users (and some for test_user_id if num_interactions is high)
    for _ in range(num_interactions - specific_interactions_count): # Adjust count for specific interactions
        user = random.choice(sample_users_list)
        pet = random.choice(sample_pets_list)
        action = random.choice(interaction_types)
        db.child("user_interactions").child(str(uuid.uuid4())).set({
            "user_id": user,
            "pet_id": pet,
            "interaction_type": action,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })
    print("\nGeneral sample interactions populated.")

# ------------------- Load Firebase Data -------------------
def fetch_firebase_data():
    """Fetches user, pet, and interaction data from Firebase."""
    try:
        users = db.child("users").get().val()
        pets = db.child("pets").get().val()
        interactions = db.child("user_interactions").get().val()
        # Convert Firebase dicts to DataFrames, handling None for empty nodes
        users_df = pd.DataFrame(users).T if users else pd.DataFrame()
        pets_df = pd.DataFrame(pets).T if pets else pd.DataFrame()
        interactions_df = pd.DataFrame(interactions).T if interactions else pd.DataFrame()

        # Explicitly convert numerical columns in pets_df to numeric
        numerical_cols_to_convert = ['AgeMonths', 'WeightKg', 'TimeInShelterDays', 'AdoptionFee']
        for col in numerical_cols_to_convert:
            if col in pets_df.columns:
                # Use errors='coerce' to turn non-numeric values into NaN, then fill NaN with 0
                pets_df[col] = pd.to_numeric(pets_df[col], errors='coerce').fillna(0)

        return users_df, pets_df, interactions_df
    except Exception as e:
        print(f"Error fetching data from Firebase: {e}")
        traceback.print_exc()
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

# ------------------- Content-Based Filtering Functions -------------------
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def prepare_content_data(pets_df):
    """
    Prepares pet data for content-based similarity calculation, including binning AgeMonths.
    Returns a dictionary of Series, one for each feature, ready for TF-IDF.
    """
    pets_df_copy = pets_df.copy()

    # Ensure all relevant columns exist, fill missing with empty string for categorical
    feature_cols = ['PetType', 'Breed', 'Color', 'Size', 'AgeMonths']
    for col in feature_cols:
        if col not in pets_df_copy.columns:
            if col == 'AgeMonths':
                pets_df_copy[col] = 0 # Default numerical for AgeMonths
            else:
                pets_df_copy[col] = '' # Default empty string for categorical

    # Ensure AgeMonths is numeric before binning
    pets_df_copy['AgeMonths'] = pd.to_numeric(pets_df_copy['AgeMonths'], errors='coerce').fillna(0)

    # Bin AgeMonths into categories
    bins = [0, 12, 60, 200] # Example bins: 0-12 months (young), 1-5 years (adult), 5+ years (senior)
    labels = ['young', 'adult', 'senior']
    pets_df_copy['AgeMonths_binned'] = pd.cut(pets_df_copy['AgeMonths'], bins=bins, labels=labels, right=False, include_lowest=True).astype(str)

    # Prepare individual feature series for TF-IDF
    prepared_features = {
        'PetType': pets_df_copy['PetType'].astype(str),
        'Breed': pets_df_copy['Breed'].astype(str),
        'Color': pets_df_copy['Color'].astype(str),
        'Size': pets_df_copy['Size'].astype(str),
        'AgeMonths_binned': pets_df_copy['AgeMonths_binned'].astype(str)
    }
    return prepared_features, pets_df_copy.index.tolist() # Also return the index for mapping

def compute_content_similarity_matrix(prepared_features, pets_index):
    """
    Computes a weighted cosine similarity matrix based on prioritized pet features.
    Args:
        prepared_features (dict): Dictionary of Series, one for each feature.
        pets_index (list): List of pet IDs corresponding to the order in prepared_features.
    Returns:
        np.array: Weighted content similarity matrix.
    """
    if not prepared_features or not pets_index:
        print("Warning: No prepared features or pet index for content similarity calculation.")
        return np.array([[]]), {}

    # Define weights for each feature based on your desired prioritization
    # These weights should sum to 1.0
    feature_weights = {
        'PetType': 0.4,
        'Breed': 0.3,
        'Color': 0.15,
        'Size': 0.1,
        'AgeMonths_binned': 0.05
    }

    # Initialize a zero matrix for the combined similarity
    num_pets = len(pets_index)
    combined_sim_matrix = np.zeros((num_pets, num_pets))

    # Store TF-IDF vectorizers to reuse for recommendations
    tfidf_vectorizers = {}

    for feature_name, weight in feature_weights.items():
        if feature_name in prepared_features and not prepared_features[feature_name].empty:
            # Ensure there's actual content to fit TF-IDF
            if prepared_features[feature_name].str.strip().eq('').all():
                print(f"Warning: All values for '{feature_name}' are empty. Skipping TF-IDF for this feature.")
                continue

            tfidf = TfidfVectorizer(stop_words='english')
            tfidf_matrix = tfidf.fit_transform(prepared_features[feature_name])

            # Store the fitted TF-IDF vectorizer
            tfidf_vectorizers[feature_name] = tfidf

            # Calculate cosine similarity for this feature
            feature_sim_matrix = cosine_similarity(tfidf_matrix)

            # Add to the combined matrix with its weight
            combined_sim_matrix += (feature_sim_matrix * weight)
        else:
            print(f"Warning: Feature '{feature_name}' not found in prepared features or is empty. Skipping.")

    # Ensure the combined_sim_matrix is not all zeros if no features were processed
    if np.all(combined_sim_matrix == 0):
        print("Warning: Combined similarity matrix is all zeros. Check data and feature weights.")
        return np.array([[]]), {} # Return empty dict for vectorizers too

    return combined_sim_matrix, tfidf_vectorizers # Return vectorizers for potential future use

def recommend_content_based(pet_id, pets_df, content_sim_matrix, top_n=5):
    """
    Recommends pets based purely on content similarity to a given pet.
    Args:
        pet_id (str): The ID of the pet to find similar ones for.
        pets_df (pd.DataFrame): DataFrame of all pets.
        content_sim_matrix (np.array): Pre-computed content similarity matrix.
        top_n (int): Number of top recommendations to return.
    Returns:
        pd.DataFrame: DataFrame of recommended pets.
    """
    if pets_df.empty or content_sim_matrix.size == 0:
        print("Content-based recommendation skipped: pets_df is empty or content_sim_matrix is not computed.")
        return pd.DataFrame()

    if pet_id not in pets_df.index:
        print(f"Pet ID '{pet_id}' not found in the pets DataFrame for content-based recommendation.")
        return pd.DataFrame()

    idx = pets_df.index.get_loc(pet_id)
    sim_scores = list(enumerate(content_sim_matrix[idx]))
    # Sort the pets based on the similarity scores in descending order
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # Get the scores of the top_n similar pets (excluding itself)
    # Ensure we don't go out of bounds if there are fewer than top_n+1 pets
    sim_scores = [score for score in sim_scores if score[0] != idx][:top_n]

    if not sim_scores:
        return pd.DataFrame()

    pet_indices = [i[0] for i in sim_scores]
    return pets_df.iloc[pet_indices]

# ------------------- Collaborative Filtering Functions -------------------
from sklearn.preprocessing import MinMaxScaler

def prepare_collaborative_data(interactions_df):
    """Prepares interaction data for collaborative filtering."""
    if interactions_df.empty:
        return pd.DataFrame() # Return empty DataFrame directly
    interactions_df_copy = interactions_df.copy()
    # Use 'interaction_type' to infer a rating, e.g., 'adopted' > 'like' > 'view'
    # For this example, we'll stick to 1.0 as in your original script.
    interactions_df_copy['rating'] = 1.0
    # Drop duplicates for clean user-item matrix (e.g., multiple views of same pet by same user)
    interactions_df_copy = interactions_df_copy.drop_duplicates(subset=['user_id', 'pet_id'])

    # Create the user-item interaction matrix
    # Ensure user_id and pet_id are correctly set as index and columns
    interaction_matrix = interactions_df_copy.pivot_table(
        index='user_id',
        columns='pet_id',
        values='rating',
        fill_value=0
    )
    return interaction_matrix

def compute_user_similarity_matrix(interaction_matrix):
    """Computes cosine similarity matrix between users."""
    if interaction_matrix.empty:
        return pd.DataFrame()
    # Check if there's enough data to compute similarity (at least 2 users)
    if interaction_matrix.shape[0] < 2:
        print("Warning: Not enough users in interaction matrix to compute user similarity.")
        return pd.DataFrame(index=interaction_matrix.index, columns=interaction_matrix.index)

    user_sim = cosine_similarity(interaction_matrix)
    return pd.DataFrame(user_sim, index=interaction_matrix.index, columns=interaction_matrix.index)

def recommend_collaborative_based(user_id, pets_df, interaction_matrix, user_similarity_df, top_n=5):
    """
    Recommends pets based purely on collaborative filtering for a given user.
    Args:
        user_id (str): The ID of the user to get recommendations for.
        pets_df (pd.DataFrame): DataFrame of all pets.
        interaction_matrix (pd.DataFrame): User-item interaction matrix.
        user_similarity_df (pd.DataFrame): User-user similarity matrix.
        top_n (int): Number of top recommendations to return.
    Returns:
        pd.DataFrame: DataFrame of recommended pets.
    """
    if user_id not in user_similarity_df.index:
        print(f"User ID '{user_id}' not found in interactions for collaborative filtering. Cannot provide recommendations.")
        return pd.DataFrame()
    if interaction_matrix.empty:
        print("Collaborative recommendation skipped: interaction_matrix is empty.")
        return pd.DataFrame()

    # Get the similarity scores for the target user with all other users
    user_sim_scores = user_similarity_df.loc[user_id]

    # Predict ratings for all pets for the target user
    # This is a weighted sum of ratings from similar users
    # We use dot product: user_similarity_vector @ interaction_matrix
    # Handle potential misalignment if user_sim_scores and interaction_matrix columns/rows don't match
    # Ensure alignment by reindexing user_sim_scores if necessary
    aligned_user_sim_scores = user_sim_scores.reindex(interaction_matrix.index, fill_value=0)

    # Perform dot product, ensuring correct matrix dimensions
    predicted_ratings = aligned_user_sim_scores.values @ interaction_matrix.values

    # Create a Series of predicted ratings indexed by pet_id
    predicted_ratings_series = pd.Series(predicted_ratings, index=interaction_matrix.columns)

    # Get pets the user has already interacted with
    interacted_pets = interaction_matrix.loc[user_id][interaction_matrix.loc[user_id] > 0].index.tolist()

    # Filter out pets the user has already interacted with
    uninteracted_pets_ratings = predicted_ratings_series.drop(interacted_pets, errors='ignore')

    # Sort pets by predicted rating and get top N
    recommended_pet_ids = uninteracted_pets_ratings.nlargest(top_n).index

    # Filter pets_df to only include the recommended pet IDs that actually exist in pets_df
    final_recommended_pets = pets_df.loc[pets_df.index.intersection(recommended_pet_ids)]

    return final_recommended_pets

# ------------------- Hybrid Recommendation Function -------------------
def hybrid_recommend(user_id, pets_df, content_sim_matrix, interaction_matrix, user_similarity_df, top_n=5,
                     collab_weight=0.5, content_weight=0.5, pets_index_for_content_matrix=None, tfidf_vectorizers=None):
    """
    Generates hybrid recommendations for a given user.
    Combines collaborative filtering scores and content-based scores.
    Args:
        user_id (str): The ID of the user to get recommendations for.
        pets_df (pd.DataFrame): DataFrame of all pets.
        content_sim_matrix (np.array): Pre-computed content similarity matrix (weighted by features).
        interaction_matrix (pd.DataFrame): User-item interaction matrix.
        user_similarity_df (pd.DataFrame): User-user similarity matrix.
        top_n (int): Number of top recommendations to return.
        collab_weight (float): Weight for collaborative filtering score.
        content_weight (float): Weight for content-based filtering score.
        pets_index_for_content_matrix (list): Original index of pets_df when content_sim_matrix was computed.
        tfidf_vectorizers (dict): Dictionary of fitted TfidfVectorizers for each feature.
    Returns:
        pd.DataFrame: DataFrame of recommended pets.
    """
    if user_id not in user_similarity_df.index:
        print(f"User ID '{user_id}' not found in interactions for hybrid filtering. Cannot provide recommendations.")
        return pd.DataFrame()
    if interaction_matrix.empty or content_sim_matrix.size == 0:
        print("Hybrid recommendation skipped: interaction_matrix or content_sim_matrix is empty.")
        return pd.DataFrame()
    if pets_index_for_content_matrix is None or tfidf_vectorizers is None:
        print("Hybrid recommendation skipped: missing content matrix index or TF-IDF vectorizers.")
        return pd.DataFrame()

    # Get ALL pets the user has already interacted with (for filtering out from final recommendations)
    user_interacted_pets_ids = interaction_matrix.loc[user_id][interaction_matrix.loc[user_id] > 0].index.tolist()


    # --- Collaborative Filtering Part ---
    user_sim_scores = user_similarity_df.loc[user_id]
    aligned_user_sim_scores = user_sim_scores.reindex(interaction_matrix.index, fill_value=0)
    collab_raw_scores = aligned_user_sim_scores.values @ interaction_matrix.values

    if collab_raw_scores.max() > collab_raw_scores.min(): # Avoid division by zero if all scores are same
        collab_scores_normalized = MinMaxScaler().fit_transform(collab_raw_scores.reshape(-1, 1)).flatten()
    else:
        collab_scores_normalized = np.zeros_like(collab_raw_scores)

    collab_scores_series = pd.Series(collab_scores_normalized, index=interaction_matrix.columns)

    # Get pets the user has already positively interacted with (for building content profile)
    user_positive_interacted_pets = interactions_df[
        (interactions_df['user_id'] == user_id) &
        (interactions_df['interaction_type'].isin(['like', 'adopted']))
    ]['pet_id'].unique().tolist()

    # --- Content-Based Filtering Part (User-Specific Profile) ---
    content_scores_for_hybrid_pets = {}
    candidate_pet_ids = interaction_matrix.columns # All pets that have appeared in interactions

    # Build user's content profile by averaging TF-IDF vectors of positively interacted pets
    user_profile_vectors = {}
    feature_weights = { # Must match weights used in compute_content_similarity_matrix
        'PetType': 0.4,
        'Breed': 0.3,
        'Color': 0.15,
        'Size': 0.1,
        'AgeMonths_binned': 0.05
    }

    # Ensure all feature names are consistent
    feature_names_for_profile = ['PetType', 'Breed', 'Color', 'Size', 'AgeMonths_binned']

    if user_positive_interacted_pets:
        for feature_name in feature_names_for_profile:
            if feature_name in tfidf_vectorizers:
                # Get the relevant data for the user's positive interactions for this feature
                interacted_pets_data = pets_df.loc[pets_df.index.intersection(user_positive_interacted_pets)].copy()

                # Re-prepare age months binned if not already done for this subset
                if feature_name == 'AgeMonths_binned' and 'AgeMonths' in interacted_pets_data.columns:
                    bins = [0, 12, 60, 200]
                    labels = ['young', 'adult', 'senior']
                    # Ensure AgeMonths is numeric before binning
                    interacted_pets_data['AgeMonths'] = pd.to_numeric(interacted_pets_data['AgeMonths'], errors='coerce').fillna(0)
                    interacted_pets_data['AgeMonths_binned'] = pd.cut(interacted_pets_data['AgeMonths'], bins=bins, labels=labels, right=False, include_lowest=True).astype(str)
                elif feature_name not in interacted_pets_data.columns:
                     interacted_pets_data[feature_name] = '' # Ensure column exists

                # Transform these features into TF-IDF vectors using the fitted vectorizer
                feature_vectors = tfidf_vectorizers[feature_name].transform(interacted_pets_data[feature_name].astype(str))

                # Average the vectors to create a part of the user's content profile
                user_profile_vectors[feature_name] = np.mean(feature_vectors.toarray(), axis=0)
            else:
                user_profile_vectors[feature_name] = np.array([]) # Empty if no vectorizer

    # Calculate content similarity for each candidate pet to the user's profile
    for candidate_pet_id in candidate_pet_ids:
        # Skip pets user has already interacted with
        if candidate_pet_id in user_interacted_pets_ids: # Use all interacted pets for filtering
            content_scores_for_hybrid_pets[candidate_pet_id] = 0.0
            continue

        if candidate_pet_id not in pets_df.index:
            content_scores_for_hybrid_pets[candidate_pet_id] = 0.0
            continue

        candidate_pet_data = pets_df.loc[candidate_pet_id].copy()

        # Re-prepare age months binned for candidate pet
        if 'AgeMonths' in candidate_pet_data and 'AgeMonths_binned' not in candidate_pet_data:
            bins = [0, 12, 60, 200]
            labels = ['young', 'adult', 'senior']
            # Ensure AgeMonths is numeric before binning
            age_months_numeric = pd.to_numeric(candidate_pet_data['AgeMonths'], errors='coerce')
            if pd.isna(age_months_numeric):
                age_months_numeric = 0 # Default to 0 or another suitable value

            # The error was here: .iloc[0].astype(str) was trying to call .astype(str) on a string
            # pd.cut with labels already returns string-like categories.
            candidate_pet_data['AgeMonths_binned'] = pd.cut(pd.Series([age_months_numeric]), bins=bins, labels=labels, right=False, include_lowest=True).iloc[0]

        total_content_sim = 0.0
        for feature_name, weight in feature_weights.items():
            if feature_name in tfidf_vectorizers and feature_name in user_profile_vectors and user_profile_vectors[feature_name].size > 0:
                # Transform candidate pet's feature using the same TF-IDF vectorizer
                candidate_feature_vector = tfidf_vectorizers[feature_name].transform([str(candidate_pet_data.get(feature_name, ''))]).toarray()

                # Calculate cosine similarity between candidate pet's feature vector and user's profile for this feature
                if candidate_feature_vector.size > 0:
                    sim = cosine_similarity(candidate_feature_vector, user_profile_vectors[feature_name].reshape(1, -1))[0][0]
                    total_content_sim += sim * weight

        content_scores_for_hybrid_pets[candidate_pet_id] = total_content_sim

    content_scores_series = pd.Series(content_scores_for_hybrid_pets)

    # Normalize these content scores
    if not content_scores_series.empty and content_scores_series.max() > content_scores_series.min():
        content_scores_normalized = MinMaxScaler().fit_transform(content_scores_series.values.reshape(-1, 1)).flatten()
        content_scores_series = pd.Series(content_scores_normalized, index=content_scores_series.index)
    else:
        content_scores_series = pd.Series(0.0, index=candidate_pet_ids)


    # --- Combine Scores ---
    # Ensure indices align before combining
    common_index = collab_scores_series.index.intersection(content_scores_series.index)
    collab_scores_aligned = collab_scores_series.reindex(common_index, fill_value=0)
    content_scores_aligned = content_scores_series.reindex(common_index, fill_value=0)

    combined_scores = (collab_scores_aligned * collab_weight) + (content_scores_aligned * content_weight)

    # Filter out pets the user has already interacted with (re-using user_interacted_pets_ids)
    uninteracted_pets_scores = combined_scores.drop(user_interacted_pets_ids, errors='ignore')

    # Get top N recommended pets
    recommended_pet_ids = uninteracted_pets_scores.nlargest(top_n).index

    # Filter pets_df to only include the recommended pet IDs that actually exist in pets_df
    final_recommended_pets = pets_df.loc[pets_df.index.intersection(recommended_pet_ids)]

    return final_recommended_pets

# ------------------- Predict Adoption Likelihood -------------------
def predict_adoption_likelihood(pets_df_subset):
    """
    Predicts the adoption likelihood for a given DataFrame of pets.
    Requires 'encoder', 'scaler', 'categorical_cols', 'numerical_cols' to be loaded.
    This version ensures feature names are maintained to avoid UserWarning.
    """
    if pets_df_subset.empty:
        return np.array([])
    try:
        # Ensure the subset has all expected categorical and numerical columns, filling missing with empty string/0
        for col in categorical_cols:
            if col not in pets_df_subset.columns:
                pets_df_subset[col] = ''
        for col in numerical_cols:
            if col not in pets_df_subset.columns:
                pets_df_subset[col] = 0.0 # Or a suitable default numerical value

        # Align columns with those used during training
        pets_subset_aligned_cat = pets_df_subset[categorical_cols]
        pets_subset_aligned_num = pets_df_subset[numerical_cols]

        # Transform data
        cat_data_transformed = encoder.transform(pets_subset_aligned_cat)
        num_data_scaled = scaler.transform(pets_subset_aligned_num)

        # Create DataFrame from transformed data to preserve feature names for prediction
        # Get feature names from encoder (for categorical) and numerical_cols (for numerical)
        all_feature_names = numerical_cols + list(encoder.get_feature_names_out(categorical_cols))

        # Combine numerical and categorical transformed data into a DataFrame with correct feature names
        X_predict = pd.DataFrame(
            np.hstack([num_data_scaled, cat_data_transformed]),
            columns=all_feature_names,
            index=pets_df_subset.index # Keep original index
        )

        preds = model.predict_proba(X_predict)[:, 1] # Probability of the positive class (AdoptionLikelihood = 1)
        return preds
    except Exception as e:
        print(f"Error during adoption likelihood prediction: {e}")
        traceback.print_exc()
        return np.array([])

# ------------------- Main Execution -------------------
if __name__ == '__main__':
    try:
        # --- Data Preparation and Population (Run this once to set up Firebase) ---
        # Load pet_adoption_data.csv for populating 'pets' node in Firebase
        try:
            full_pets_data = pd.read_csv('pet_adoption_data.csv')
            # Use a subset of columns that are relevant for pet attributes
            # Ensure 'features' column is handled correctly if it exists or needs to be created
            if 'features' not in full_pets_data.columns:
                 # Create a simple 'features' column for demonstration
                 full_pets_data['features'] = full_pets_data[['PetType', 'Breed', 'Color', 'Size']].astype(str).agg(' '.join, axis=1)

            # Use a subset of the full dataset for populating Firebase 'pets' to keep it manageable
            # For example, take the first 100 pets
            pets_for_firebase = full_pets_data.head(100).copy()
            # Set index to a string for Firebase keys
            pets_for_firebase.index = [f"pet{i:03d}" for i in range(1, len(pets_for_firebase) + 1)]

        except FileNotFoundError:
            print("Error: 'pet_adoption_data.csv' not found. Please ensure it's in the same directory.")
            exit()

        # Uncomment the following line if you want to clear existing data before populating
        # clear_firebase_data()

        # Populate sample users and pets if they don't exist
        # Fetch current users and pets to avoid re-populating if already there
        current_users_check, current_pets_check, _ = fetch_firebase_data()

        sample_user_ids = [f"user{i:03d}" for i in range(1, 16)] # 15 sample users
        sample_pet_ids = pets_for_firebase.index.tolist() # IDs from the loaded CSV

        if current_users_check.empty or len(current_users_check) < len(sample_user_ids):
            populate_sample_users(len(sample_user_ids))
        else:
            print("Users already exist in Firebase. Skipping population.")
            sample_user_ids = current_users_check.index.tolist() # Use existing users

        if current_pets_check.empty or len(current_pets_check) < len(sample_pet_ids):
            populate_sample_pets(pets_for_firebase)
        else:
            print("Pets already exist in Firebase. Skipping population.")
            sample_pet_ids = current_pets_check.index.tolist() # Use existing pets

        # Populate sample interactions (this will clear user007's previous interactions)
        populate_sample_interactions(sample_user_ids, sample_pet_ids, num_interactions=200, test_user_id="user007")

        # --- Main Recommendation Logic ---
        user_id_to_recommend_for = "user007" # Example user ID for recommendations
        pet_id_for_content_recs = "pet080" # Example pet ID for content-based recommendations

        users_df, pets_df, interactions_df = fetch_firebase_data()
        print(f"\n--- Data Fetched ---")
        print(f"Users: {len(users_df)}, Pets: {len(pets_df)}, Interactions: {len(interactions_df)}")

        # Print user007's actual interactions for verification
        print(f"\n--- Interactions for {user_id_to_recommend_for} (for cross-verification) ---")
        user007_interactions = interactions_df[interactions_df['user_id'] == user_id_to_recommend_for]
        if not user007_interactions.empty:
            # Merge with pets_df to get pet details
            user007_interactions_details = user007_interactions.merge(
                pets_df[['PetType', 'Breed', 'Size']],
                left_on='pet_id',
                right_index=True,
                how='left'
            )
            print(user007_interactions_details[['pet_id', 'PetType', 'Breed', 'Size', 'interaction_type', 'timestamp']])
        else:
            print(f"No interactions found for {user_id_to_recommend_for}.")
        print("--------------------------------------------------")


        # Ensure indices and IDs are string type for consistency
        # Ensure pets_df index is string
        if not pets_df.empty:
            pets_df.index = pets_df.index.astype(str)
        # Ensure 'user_id' and 'pet_id' in interactions_df are string
        if not interactions_df.empty:
            if 'user_id' in interactions_df.columns:
                interactions_df['user_id'] = interactions_df['user_id'].astype(str)
            if 'pet_id' in interactions_df.columns:
                interactions_df['pet_id'] = interactions_df['pet_id'].astype(str)


        # Pre-compute matrices for efficiency
        # Pass the pets_df_prepared_content and pets_index to compute_content_similarity_matrix
        pets_df_prepared_content, pets_index_for_content_matrix = prepare_content_data(pets_df)
        content_sim_matrix, tfidf_vectorizers_for_content = compute_content_similarity_matrix(pets_df_prepared_content, pets_index_for_content_matrix)

        interaction_matrix = prepare_collaborative_data(interactions_df)
        user_similarity_df = compute_user_similarity_matrix(interaction_matrix)

        # --- Display Content-Based Recommendations ---
        print("\n" + "="*50)
        print(f"Content-Based Recommendations for Pet ID: {pet_id_for_content_recs}")
        print("="*50)
        content_recs = recommend_content_based(pet_id_for_content_recs, pets_df, content_sim_matrix)
        if not content_recs.empty:
            # Ensure 'AdoptionLikelihood' column is added to a copy of the DataFrame
            content_recs_with_likelihood = content_recs.copy()
            content_recs_with_likelihood['AdoptionLikelihood'] = predict_adoption_likelihood(content_recs_with_likelihood)
            print(content_recs_with_likelihood[[
                'PetType', 'Breed', 'Color', 'AgeMonths', 'Size', 'AdoptionLikelihood'
            ]])
        else:
            print(f"No content-based recommendations found for pet '{pet_id_for_content_recs}'.")

        # --- Display Collaborative Filtering Recommendations ---
        print("\n" + "="*50)
        print(f"Collaborative Filtering Recommendations for User ID: {user_id_to_recommend_for}")
        print("="*50)
        collab_recs = recommend_collaborative_based(user_id_to_recommend_for, pets_df, interaction_matrix, user_similarity_df)
        if not collab_recs.empty:
            collab_recs_with_likelihood = collab_recs.copy()
            collab_recs_with_likelihood['AdoptionLikelihood'] = predict_adoption_likelihood(collab_recs_with_likelihood)
            print(collab_recs_with_likelihood[[
                'PetType', 'Breed', 'Color', 'AgeMonths', 'Size', 'AdoptionLikelihood'
            ]])
        else:
            print(f"No collaborative-based recommendations found for user '{user_id_to_recommend_for}'. "
                  f"Ensure '{user_id_to_recommend_for}' has interactions in Firebase.")

        # --- Display Hybrid Recommendations (Optional, but good for comparison) ---
        print("\n" + "="*50)
        print(f"Hybrid Recommendations for User ID: {user_id_to_recommend_for}")
        print("="*50)
        hybrid_recs = hybrid_recommend(user_id_to_recommend_for, pets_df, content_sim_matrix, interaction_matrix, user_similarity_df,
                                       pets_index_for_content_matrix=pets_index_for_content_matrix, tfidf_vectorizers=tfidf_vectorizers_for_content)
        if not hybrid_recs.empty:
            hybrid_recs_with_likelihood = hybrid_recs.copy()
            hybrid_recs_with_likelihood['AdoptionLikelihood'] = predict_adoption_likelihood(hybrid_recs_with_likelihood)
            print(hybrid_recs_with_likelihood[[
                'PetType', 'Breed', 'Color', 'AgeMonths', 'Size', 'AdoptionLikelihood'
            ]])
        else:
            print(f"No hybrid recommendations found for user '{user_id_to_recommend_for}'.")

    except Exception as e:
        print("An unexpected error occurred during main execution:\n", traceback.format_exc())
